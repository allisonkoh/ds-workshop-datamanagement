---
title: "Hertie School/SCRIPTS Data Science Workshop Series"
subtitle: "Session 2: Modern data management with R"
author:
- Therese Anders^[Instructor, Hertie School/SCRIPTS, anders@hertie-school.org.]
- Allison Koh^[Teaching Assistant, Hertie School, kohallison3@gmail.com.]
date: January 17, 2020
output: pdf_document
---

 
# Introduction
Transforming and combining data from different sources into formats suitable for statistical analysis and visualization lies at the heart of the data science workflow. In many data science applications, an estimated 50% to 80% of a project is spent on data collection, cleaning, and wrangling. Today, we will be introducing the `dplyr`, `tidyr`, and `lubridate` packages. Together with `stringr` (string operations using regular expressions), these packages offer functionality for the majority of data cleaning and reshaping task in `R`.

RStudio has a number of great cheat sheets available online: https://rstudio.com/resources/cheatsheets/. Check out the "Data transformation with `dplyr` cheat sheet!


## The data

For this exercise, we will be using data from Five Thirty Eight's [Presidential Endorsement Tracker](https://fivethirtyeight.com/methodology/how-our-presidential-endorsement-tracker-works/), which records endorsements of American presidential primary candidates from current/former government officials and high-ranked political party members (i.e. members of the Democratic National Convention). For this dataset, an endorsement is defined as a "public display or pronouncement of support that articulates or strongly implies that a candidate is an endorser's current No. 1 choice for president". This means that there is only one endorsement linked to each endorser, and that they can change over the course of the primary season. In the case of ambiguities, Five Thirty Eight reaches out to offices of the candidates and endorsers for a clarification. 

In this model, endorsements are "ranked" by their perceived relative value based on their current/former position. For the 2020 Democratic Presidential Primaries, it comprises of the following: 

1. Current and former presidents and vice presidents (10 points).
2. Current party leaders: Nancy Pelosi (House speaker), Steny Hoyer (House majority leader), James Clyburn (House majority whip), Chuck Schumer (Senate minority leader), Dick Durbin (Senate minority whip) and Tom Perez (Democratic National Committee chair) (10 points).
3. Current governors, including governor equivalents from the U.S. territories and Washington, D.C.1 (8 points).
4. Current U.S. Senators (6 points).
5. Past presidential and vice presidential nominees (5 points).
6. Former party leaders2 (5 points).
7. Former 2020 presidential candidates who appeared in at least one debate and have since dropped out (5 points).
8. Current U.S. representatives, including non-voting delegates from U.S. territories (3 points).
9. Mayors of cities with at least 300,000 people (3 points).
10. Officials holding statewide or territory-wide elected office, excluding positions (e.g. commissioners) that are held by multiple people at once (2 points).
11. State and territorial legislaturesâ€™ majority and minority leaders (2 points).3
12. Other DNC members (1 point).

The dataset contains the following variables: 

* `year`: year endorsement took place 
* `endorser`: individual who is endorsing a presidential candidate 
* `position`: endorser's position in government 
* `state`: origin state of endorser 
* `endorsee`: presidential candidate endorsed 
* `endorser.party`: political party of the endorser
* `source`: link to endorsement (if online)
* `points`: # of "endorsement points" attributed to the endorser (see above for more details)
* `drop`: Has the candidate dropped out of the race? (1=yes)
* `drop_date`: date that the candidate dropped out 

```{r}
library(foreign)
raw <- read.csv("538_presidential_endorsements.csv",
               stringsAsFactors = FALSE)
str(raw)
```

## Functions in `dplyr`
`dplyr` uses a strategy called "Split - Apply - Combine". Some of the key functions include:

* `select()`: Subset columns.
* `filter()`: Subset rows.
* `arrange()`: Reorders rows.
* `mutate()`: Add columns to existing data.
* `summarise()` or `summarize()`: Summarizing the data set.

First, lets download the package and call it using the `library()` function.
```{r, message = F, warning=F}
# install.packages("dplyr")
library(dplyr)
```

## Using `select()` and introducing the Piping Operator `%>%`
Using the so-called **piping operator** will make the `R` code faster and more legible, because we are not saving every output in a separate data frame, but passing it on to a new function. First, let's use only a subsample of variables in the data frame.

Notice a couple of things in the code below:

* We can assign the output to a new data set.
* We use the piping operator to connect commands and create a single flow of operations.
* We can use the select function to rename variables.
* Instead of typing each variable, we can select sequences of variables.
* Note that the `everything()` command inside `select()` will select all variables.

**NOTE:** The `::` operator specifies that we want to use the *object* `select()` from the *package* `dplyr`. This explicit programming is useful when functions or  objects are contained in multiple packages to avoid confusion. In practice, I now always write `dplyr::select()` because I have come across a few instances where code broke because `select` was part of another package. 

### Selecting variables with `select()`
```{r}
# Explicitly selecting variables one-by-one
df_sub1 <- raw %>%
  dplyr::select(position,
                endorsee)

# Selecting multiple variables at once
df_sub2 <- raw %>%
  dplyr::select(position,
                endorsee,
                source:drop_date) #Selecting a number of columns. 
names(df_sub2)
# Later we will also learn a few additional tricks using helper functions
```

### Explicitly dropping variables with `select()`
Suppose, we didn't really want to select the `source` variable. We can use `select()` to drop variables.
```{r}
df_sub3 <- raw %>%
  dplyr::select(-source)
```

### Reordering variables with `select()`
```{r}
df_sub3 <- raw %>%
  dplyr::select(year,
                endorsee,
                endorser,
                points)
```

### Renaming variables with `select()`
We can also use `select()` to rename variables. Below, we need to use the `everything()` function if we don't want to drop the remaining variables in the process
```{r}
names(df)
df <- raw %>%
  dplyr::select(endorser_party = endorser.party,
                everything())
```


## Introducting `filter()`
While `select()` operates on columns, `filter()` operates on observations (rows). We always use `filter()` in combination with logical statements. First, let's briefly review the most commonly used logical statements from last week.  

* `x < y`
* `x <= y`
* `x == y`
* `x != y`
* `x >= y`
* `x > y`
* `x %in% c(a, b, c)` is `TRUE` if `x` is in the vector `c(a, b, c)`.

There are two more important logical statements we frequently use in data wrangling.
* `is.na()`
* `!is.na()`

First, let's take a look at the `endorser_party` variable we created above using the `table()` function. Unsurprisingly, most endorsements come from Democrats.

Who the non-Democrat endorsers are. We can use `filter()` to do so.
```{r}
table(df$endorser_party) 
df_nondem <- df %>%
  filter(endorser_party != "D")
```

Suppose, we wanted to filter all the endorsements that come from states that will take part in [Super Tuesday](https://www.uspresidentialelectionnews.com/2020-presidential-primary-schedule-calendar/super-tuesday-2020/). Registered Democrats from these states will vote for the Democratic nominee, who will be on the ballot for the 2020 Presidential Election. 

The states that will hold primary elections on this year's Super Tuesday (March 3, 2020) are Alabama (AL), Arkansas (AR), California (CA), Colorado (CO), Maine (ME), Massachussets (MA), Minnesota (MN), North Carolina (NC), Oklahoma (OK), Tennessee (TN), Texas (TX), Utah (UT), Vermont (VT), and Virginia (VA). 

```{r, warning = F}
super_tuesday <- c("AL", "AR", "CA", "CO", "ME", "MA", "MN", "NC", "OK", "TN", "TX", "UT", "VT", "VA")
df_super <- df %>%
  filter(state %in% super_tuesday)
```

Wait, what do we need `%in%` for? Couldn't we just use `==`? Lets use a smaller example to illustrate why this isn't the case. Below, lets select only the following states: California (CA) and Texas (TX).
```{r}
head(df$state)
test1 <- df %>%
  filter(state %in% c("CA", "TX"))
nrow(test1)
head(test1$state)

test2 <- df %>%
  filter(state == c("CA", "TX"))
nrow(test2)
head(test2$state)
```

What does `R` do? Well it does a vector matching operation. From the variable `df$state` (`"MD" "NY" "CA" "DE" "TX" "CA"`) it test is the first element "CA", is the second element "TX", is the third element "CA", the fourth element "TX", and so on. Only if the return for each of these checks is `TRUE` does it select the respective row (observation).

## Helper functions
`dplyr` has a number of helper functions--and that is where the magic lies. These can be used with either `select()` or `filter()`. Here are some useful functions:

* `starts_with()`
* `ends_with()`
* `contains()`
* `one_of()`: Every name that appears in x, which should be a character vector.

For example, let's create a data frame with all variables that contain the word "endorse".
```{r}
test <- df %>%
  dplyr::select(contains("endorse"))
```

## Introducting `mutate()`
Suppose we want to rescale the `points` variable to run from 0 to 1. Since the original variable is scaled from 1 (least valuable endorsement) to 10 (most valuable endorsement; former presidents and vice presidents; current party leaders), we can create a new variable `endorsement.points.rescaled` by dividing the original variable by 10. 
```{r}
df2 <- df %>%
  dplyr::mutate(points_rescaled = points/10)
```

Suppose I only wanted to look at endorsements for candidates who are still in the race, but it is not updated to include the candidate who most recently dropped out of the race (Cory Booker). I can create a new binary variable that codes that this candidate has recently dropped out of the race. Here, we introduce the `ifelse()` function that is tremendously helpful in data wrangling exercises. The syntax of `ifelse()` is as follows: 

`ifelse(condition, if TRUE this, if FALSE this)`.

```{r}
df3 <- df2 %>% 
  dplyr::mutate(drop_new = ifelse(endorsee == "Cory Booker", 1, 0))
```

Ok, so there are a number of rows for which the `endorsee` variable is `NA`. That is weird. Lets check out how many cases we have in which the `endorsee` variable is `NA`.
```{r}
table(is.na(df$endorsee))
```

That is most of our observations! So `fivethirtyeight` appears to have already listed people it expects to announce an endorsement before they even did so! 


## Introducting `summarise()` and `arrange()`
One of the most powerful `dplyr` features is the `summarise()` function, especially in combination with `group_by()`.

First, let's compute the total number of endorsements by endorsee in the Super Tuesday states. Here, we will be using the operator `n()` to tell dplyr to count all the observations for the groups specified in `group_by()`. Also, suppose we want to know the average endorsement points attributed to each candidate. Anothter interesting variable would be the total endorsement points for each candidate to get an idea of the influence of each candidate's endorsements.

Note that, because there are missing values, we need to tell `R` what to do with them. 
```{r}
by_endorsee <- df3 %>%
  dplyr::group_by(endorsee) %>%
  dplyr::summarise(n.endorsement=dplyr::n(),
            average.endorsement.points = mean(endorsement.points, na.rm = T),
            total.endorsement.points = sum(endorsement.points))
```

To get an idea of where each candidate stands among politicians from the Super Tuesday states, lets reorder the output in descending order using `arrange()`.
```{r}
by_endorsee <- df3 %>%
  dplyr::group_by(endorsee) %>%
  dplyr::summarise(n.endorsement=dplyr::n(),
            average.endorsement.points = mean(endorsement.points, na.rm = T),
            total.endorsement.points = sum(endorsement.points)) %>% 
  dplyr::arrange(desc(n.endorsement)) #Default is ascending order

by_endorsee
```

So, as of January 14, 2020, 215 politicians from the Super Tuesday states have not endorsed a candidate for the Democratic primary elections. Among the endorsees, Joe Biden received the most endorsements and has the highest number of "total endorsement points" per [the scale of each endorsement's relative value generated by Five Thirty Eight](https://fivethirtyeight.com/methodology/how-our-presidential-endorsement-tracker-works/). However, the candidate with the highest value for "average endorsement points" is Amy Klobuchar. *Perhaps* this indicates that, on average, politicians from Super Tuesday states who have endorsed Amy Klobuchar have more influence. 

## Putting it all together: The power of piping
In this example, I am starting all the way from the original `df` dataframe to demonstrate the power of the piping operator.

```{r}
df_new <- df %>% 
  dplyr::select(year,
                position,
                endorsement.points = points, 
                state:source) %>% 
  dplyr::filter(state %in% super_tuesday) %>% 
  dplyr::mutate(endorsement.points.rescaled = endorsement.points/10,
                dropped = ifelse(endorsee %in% drop, 1, 0)) %>%
  dplyr::filter(dropped == 0) %>% 
  dplyr::group_by(endorsee) %>%
  dplyr::summarise(n.endorsement=dplyr::n(),
            average.endorsement.points = mean(endorsement.points, na.rm = T),
            total.endorsement.points = sum(endorsement.points)) %>% 
  dplyr::arrange(desc(n.endorsement)) 

df_new
```


## Saving files
Finally, lets save the output of our code. This time, we would like to save it in the .dta format. Remember to load the `foreign()` library before saving.
```{r, warning = F}
library(foreign)
write.dta(df_new, "super_tuesday_endorsements.dta")
```

# Merging data

## The data 
In `R`, the package `haven` allows us to access read and write various data formats. Currently, it supports Stata, SPSS and SAS files. Value labels from imported data are translated into a new `labelled()` class, which preserves the original semantics. In addition to some of the functions that can be used to explore the dimensions of any data frame, we can also explore the substantive descriptions of variables. 

For this exercise, we will be working with data from the Pew Research Center's *American Trends Panel* (ATP), which is a nationally representative panel of randomly selected U.S. adults. First, we will load the data from the 38th and 39th wave of the panel. The data has been cleaned after downloading from the original sources and subsetted to only include U.S. citizens and respondents who participated in both surveys.

The topics covered in each wave are as follows: 

* Wave 38: Pre-election poll, generations research 
* Wave 39: Post-election poll 

The Wave 38 data contains the following variables: 

* `POL1DT_W38`: Do you approve or disapprove of the way Donald Trump is handling his job as president?
* `VTPLAN`: Do you plan to vote in the elections this November?
* `F_PARTY_FINAL`: Party affiliation 

The Wave 39 data contains the following variables: 

* `POL1DT_W39`: Do you approve or disapprove of the way Donald Trump is handling his job as president?
* `VTHPPYUS_W39`: All in all, are you happy or unhappy with the results of the recent elections that took place across the United States?
* `VOTED_ATP`: Which of the following statements best describes you?
* `F_PARTY_FINAL`: Party affiliation 

First, let's load the data from both waves. 

```{r}
library(haven)
# load datasets 
w38.df <- haven::read_dta("ATP_W38.dta")
w39.df <- haven::read_dta("ATP_W39.dta")
```

Then, let's investigate some of the variables used in each dataset. 

```{r}
attributes(w38.df$VTPLAN_W38)
attributes(w39.df$VOTED_ATP_W39)
attributes(w38.df$F_PARTY_FINAL)
attributes(w38.df$F_IDEO)
```

**Exercise 1** Suppose we would like to investigate differences between Democrats and Republicans in the Wave 39 data. Add new variables called `dem` and `rep` that code these party affiliations using functions from the `dplyr` package and piping.

```{r, echo = F, warning = F, message = F}
library(dplyr)
w39.df <- w39.df %>%
  dplyr::mutate(DEM = ifelse(F_PARTY_FINAL==2,1,0),
                REP = ifelse(F_PARTY_FINAL==1,1,0))
head(w39.df)
```

## A primer on data merging using base `R` functions
In principle, we can merge data using the `cbind()` ("column bind") and `rbind()` ("row bind") functions, that we are already familiar with.
```{r}
vec1 <- c(1, 2, 3)
vec2 <- c("a", "a", "b")
cbind(vec1, vec2)
rbind(vec1, vec2)
```

However, this requires the data to align in its dimensions. In addition, for `cbind()` the ordering of the rows has to be the exact same to achieve correct merging; for `rbind()` the ordering of the columns has to be equal to achieve the right output.
```{r}
dat1 <- cbind(vec1, vec2)
vec3 <- c(T, F, T, T)
cbind(dat1, vec3) # The fourth observation in vec3 is omitted
obs <- c("c", 4)
rbind(dat1, obs)
```

As a side note: The `bind_rows()` function in the `dplyr` package is the smart cousin of `rbind()`. Given that two dataframes have an equal number of columns, it can merge them based on the variable name; the ordering of the columns does not matter.

## Using dplyr for merging
Luckily, the `dplyr` package in `R` contains a number of functions that allow us to merge data in "smarter" ways. Please refer to the [Data Wrangling Cheat Sheet](https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf) for an overview of the functions `dplyr` has available.

Suppose, we have two data frames: `x` (here WDI) and `y` (here COW). The basic syntax for data merging with `dplyr` is the following:

`output <- join(A, B, by = "variable")`

We will focus on the following four join functions:

* `full_join()`: Join data from `x` and `y` upon retaining all rows and values. This is the maximum join possible. Neither `x` nor `y` is the "master."
* `inner_join()`: Join only those rows that appear in both `x` and `y`. Neither `x` nor `y` is the "master."
* `left_join()`: Join only those rows from `y` that appear in `x`, retaining all data in `x`. Here, `x` is the "master."
* `right_join()`: Join only those rows from `x` that appear in `y`, retaining all data in `y`. Here, `y` is the "master."

### `full_join()`
Suppose we wanted to keep the maximum amount of data in our new merged data set. We would use the `full_join()` function to merge the Wave 38 and Wave 39 data. 
```{r}
full_wrong <- dplyr::full_join(w38.df, w39.df, by = "F_PARTY_FINAL")
nrow(full_wrong)
full_wrong
```

A couple of things to notice:

1. There are a couple of duplicate column names in our data. `dplyr` automatically added a suffix to those instances; x for the Wave 38 data (the first data set we passed to the function) and y for the Wave 39 data (the second data set passed to the function). We will rename these variables later on to keep track from which data set they originate.
2. We have over 30 million observations even though our original datasets only have approximately 10,000 observations each. Why do you think this is the case? Do you notice something weird about the `QKEY` variable?

The reason for the high number of observations is that we asked `dplyr` to join the data sets based on the `F_PARTY_FINAL` variable only. This matches each observation in dataset x to each observation in dataset y as long as they have the same party affiliation. However, the rows in our data are not uniquely identified by the `F_PARTY_FINAL` variable. Since we are working with panel data sets, each observation is uniquely identified by the variable `QKEY` to track which respondents participated in multiple waves. We need to pass this information to the `full_join()` function to correctly match up the observations in our data.

In some cases, `full_join()` and the other join functions from dplyr can often identify which columns to match on based on duplicate column names. However, in our particular case, we run into problems because we have more duplicate column names that unique identifiers for the data; see the message that R outputs when running the command without specifying by which variables to merge.

```{r}
full_alsowrong <- dplyr::full_join(w38.df, w39.df)
nrow(full_alsowrong)
```

Before we do the correct `full_join()`, let's change the name of variables that are not unique identifiers in each dataset. To do this, we can use the `select()` function. 

```{r}
w38.df <- w38.df %>% 
  select(QKEY:VTPLAN_W38,
         F_PARTY_FINAL_W38 = F_PARTY_FINAL,
         F_IDEO_W38 = F_IDEO)
names(w38.df)
```

```{r}
w39.df <- w39.df %>% 
  select(QKEY:VOTED_ATP_W39,
         F_PARTY_FINAL_W39 = F_PARTY_FINAL, 
         F_IDEO_W39 = F_IDEO)
names(w39.df)
```

Now we are ready to join! 

```{r}
merged_full <- full_join(w38.df, w39.df, by = "QKEY")
head(merged_full)
nrow(merged_full)
```

### `inner_join()`
Suppose, instead of retaining as much data as possible, we only wanted to keep data for which we have observations in both data sets. Note that this merging is only done based on the variables that we specify to uniquely identify each observation. `NA` values in all other columns are retained. Please refer to the *Data Wrangling Cheat Sheet* for methods that will select based on matching values beyond the unique identifiers (see for example `intersect()` and `setdiff()`).
```{r}
merged_inner <- inner_join(w38.df, w39.df, by = "QKEY", na.rm = T) 
head(merged_inner)
nrow(merged_inner)
```

### `left_join()` and `right_join()`
Suppose we had an existing master data set and wanted to add data to this master without adding new rows---just new columns. This could for example be the case if we had a specific time frame that we wanted to study, and only want to merge data which matches this time frame. 

Here, suppose we wanted to make the Wave 38 data our master data set and merge data from the Wave 39 data onto this master. The Wave 39 data took place after the 2018 midterm elections, while Wave 38 contains pre-election data. We are interested in keeping all of the observations from pre-election, and add on data for pre-election respondents who also participated in Wave 39. We can show that the Wave 38 data and the merged data frame have the same number of observations (rows).
```{r}
merged_left <- left_join(w38.df, w39.df, by = c("QKEY"))
nrow(w38.df) == nrow(merged_left)
```

**Exercise 2** How would you achieve the exact same result using the `right_join()` function?
```{r, echo = F}
merged_right <- right_join(w38.df, w39.df, by = c("QKEY"))
nrow(w38.df) == nrow(merged_right)
```

# Data re-shaping with `tidyr`
Another important task in data management is data re-shaping. Often, data does not come in the format that we need for data merging, data visualization, statistical analysis, or vectorized programming. In general, we want data in the following format:

1. Each variable forms a column.
2. Each observation forms a row[^2].
3. For panel data, the unit (e.g. country) and time (e.g. year) identifier form columns.

[^2]: Hadley Wickham (2014, "Tidy Data" in *Journal of Statistical Analysis*) adds another condition, namely that "Each type of observational unit forms a table." We will not go into this here, but I can highly recommend you read Wickham's piece if you want to dive deeper into tidying data.

The `tidyr` package offers two main functions for data reshaping:

* `pivot_longer()`: Shaping data from wide to long.
* `pivot_wider()`: Shaping data from long to wide.

## Wide versus long data
For **wide** data formats, each unit's responses are in a single row. For example:

| Country | Area | Pop1990 | Pop1991 |
|---------|------|---------|---------|
| A       | 300  | 56      | 58      |
| B       | 150  | 40      | 45      |

For **long** data formats, each row denotes the observation of a unit at a given point in time. For example:

| Country | Year | Area | Pop |
|---------|------|------|-----|
| A       | 1990 | 300  | 56  |
| A       | 1991 | 300  | 58  |
| B       | 1990 | 150  | 40  |
| B       | 1991 | 150  | 45  |


## `pivot_longer()`
We use the `pivot_longer()` function to reshape data from wide to long. In general, the syntax of the data is as follows:

`new_df <- pivot_longer(old_df, columns to transform, names_to = "name", values_to = "value")`

We will be working with the `merged_inner` data set. Suppose we wanted to have a single column for military expenditures, and another column identifying the data set that the data comes from.

Gathering the military expenditure variables in this way will result in duplicate rows for each country-year. Whether or not this data format is desirable depends on the intended use for the data. Most statistical analysis methods require the data to have a single row per observation (for example country-year). However, some data visualization methods fare better when each concept (for example the variable `milex`) is captured in a single column with additional columns specifying supplementary attributes of the observation (see below).

Below, note that since `tidyr` and `dplyr` are sibling packages from the "tidyverse," we can use them seamlessly in the same pipe (here using `group_by()`, `summarize()`, `mutate()`, and `filter()` from `dplyr`).
```{r, warning = F, message = F}
library(tidyr)
merged_long <- merged_inner %>%
  dplyr::group_by(F_IDEO_W39) %>% 
  dplyr::summarize(n=n(),
                   POL1DT_W38=sum(POL1DT_W38),
                   POL1DT_W39=sum(POL1DT_W39)) %>% 
  dplyr::mutate(drop = ifelse(F_IDEO_W39==99, 1, 0)) %>% # remove non-responses for political ideology question 
  dplyr::filter(drop == 0) %>% 
  tidyr::gather(variable, # name for categorical name indicator
                key, # values
                c(3:4), # variables to be reshaped
                na.rm = T)

merged_long1 <- merged_inner %>% 
  tidyr::pivot_longer(-F_IDEO_W39, names_to = "variable", values_to = "count")
merged_long
```

We can use this format to compare pre- and post-election attitudes about Donald Trump by political ideology among the panel respondents (Note: without applying survey weights, this is *not* a representative estimate). Here, I use the `ggplot2` package (also from the "tidyverse") to show how to create bar graphs that use the properties of "tidy" data, namely the fact that a specific feature is represented with a single column (here `F_IDEO_W39`), with additional columns specifying supplementary properties of this feature (here `variable`). Note that `F_IDEO_W39`==1 indicates "very conservative", while `F_IDEO_W39`==5 indicates "very liberal".

```{r, fig.width= 5, fig.height=3, warning = F}
library(ggplot2)
ggplot(merged_long, 
       aes(x = F_IDEO_W39, y = key, fill = variable)) +
  geom_bar(stat = "identity", position = "dodge2")
```

## `pivot_wider()`
Suppose we wanted to revert our operation (or generall shape data from a long to a wide format), we can use `tidyr`'s `pivot_wider()` function. The syntax is similar to `pivot_longer()`.

`new_df <- spread(old_df, key, value)`,

where `key` refers to the colum which contains the values that are to be converted to column names and `value` specifies the column that contains the value which is to be stored in the newly created columns.

```{r}
head(merged_long, 3)
merged_wide <- pivot_wider(merged_long, variable, key) 
merged_wide
```

# Sources {-}
Bacoffe, A. and Silver, N., 2019. *The 2020 Endorsement Primary*. FiveThirtyEight. Retrieved January 13, 2020.

Pew Research Center. 2015. Building Pew Research Center's American Trends Panel, Technical Report, Washington D.C. Available at http://pewrsr.ch/1Jo4nKE. 

Â© 2020 GitHub, Inc.
